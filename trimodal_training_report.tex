\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{float}
\usepackage{pgfplots}
\usepackage{tikz}
\pgfplotsset{compat=1.17}

\title{\textbf{Trimodal Radar Classification Training Report\\GitHub Actions CI Pipeline}}
\author{Radar MLOps Project}
\date{February 1, 2026}

\begin{document}

\maketitle

\section{Executive Summary}

This report presents the results of a trimodal deep learning model for automotive radar classification, successfully trained on GitHub Actions CI/CD pipeline. The model integrates \textbf{image}, \textbf{radar}, and \textbf{CSV} features using EfficientNet-B3 backbone with LSTM fusion for 3-class vehicle classification (bicycle, car, 1\_person).

\subsection{Key Achievements}
\begin{itemize}
    \item \textbf{Training Accuracy}: 97.13\%
    \item \textbf{Model Architecture}: EfficientNet-B3 + Enhanced Radar CNN + CSV Processor
    \item \textbf{Data Integration}: 2,693 samples with trimodal features
    \item \textbf{Training Duration}: 21.8 minutes (5 epochs)
    \item \textbf{CI/CD Success}: Full production pipeline on GitHub Actions
\end{itemize}

\section{Experimental Setup}

\subsection{Dataset Configuration}
\begin{table}[H]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Total Samples & 2,693 \\
Image Files & 2,693 (100\%) \\
Radar Files (.mat) & 2,686 (99.7\%) \\
CSV Files & 2,688 (99.8\%) \\
Classes & 3 (bicycle, car, 1\_person) \\
Train/Val/Test Split & 70\%/15\%/15\% \\
\bottomrule
\end{tabular}
\caption{Dataset Statistics}
\end{table}

\subsection{Model Architecture}
The trimodal fusion architecture consists of:
\begin{itemize}
    \item \textbf{Image Branch}: EfficientNet-B3 backbone (1,536 features)
    \item \textbf{Radar Branch}: 3-layer CNN (128 features)
    \item \textbf{CSV Branch}: 2-layer MLP (32→256 features)
    \item \textbf{Fusion}: 2-layer bidirectional LSTM (768→384 hidden units)
    \item \textbf{Classifier}: 4-layer MLP with graduated dropout
\end{itemize}

\subsection{Training Configuration}
\begin{table}[H]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\
\midrule
Epochs & 5 \\
Batch Size & 8 \\
Image Resolution & 224×224 px \\
Learning Rate & 2e-4 → 1e-4 \\
Weight Decay & 0.01 \\
Dropout Rate & 0.5 \\
Label Smoothing & 0.2 \\
Warmup Epochs & 3 \\
\bottomrule
\end{tabular}
\caption{Training Hyperparameters}
\end{table}

\section{Results and Analysis}

\subsection{Training Performance}

\begin{table}[H]
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Metric} & \textbf{Training} & \textbf{Validation} \\
\midrule
Accuracy & \textcolor{green}{\textbf{97.13\%}} & \textcolor{red}{38.12\%} \\
Balanced Accuracy & 97.14\% & 38.04\% \\
F1-Score (Macro) & 97.14\% & 31.70\% \\
Loss & 0.554 & 1.292 \\
Cohen's Kappa & - & 0.071 \\
ROC-AUC (OvO) & - & 0.600 \\
\bottomrule
\end{tabular}
\caption{Overall Performance Metrics}
\end{table}

\subsection{Per-Class F1 Scores}

\begin{table}[H]
\centering
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Class} & \textbf{Train F1} & \textbf{Val F1} & \textbf{Difference} \\
\midrule
1\_person & 97.05\% & 32.54\% & \textcolor{red}{-64.51\%} \\
Bicycle & 97.20\% & 12.58\% & \textcolor{red}{-84.62\%} \\
Car & 97.15\% & 50.00\% & \textcolor{red}{-47.15\%} \\
\bottomrule
\end{tabular}
\caption{Per-Class Performance Analysis}
\end{table}

\subsection{Precision and Recall Analysis}

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar,
    width=12cm,
    height=6cm,
    ylabel={Score},
    xlabel={Classes},
    ymin=0, ymax=1,
    xtick=data,
    xticklabels={1\_person, Bicycle, Car},
    legend pos=north west,
    legend style={font=\small},
]
\addplot coordinates {(0,0.459) (1,0.400) (2,0.361)};
\addplot coordinates {(0,0.252) (1,0.075) (2,0.815)};
\legend{Precision,Recall}
\end axis}
\end{tikzpicture}
\caption{Validation Precision and Recall by Class}
\end{figure}

\section{Critical Issues Identified}

\subsection{Severe Overfitting}
The model exhibits \textcolor{red}{\textbf{severe overfitting}} with a massive train-validation gap:
\begin{itemize}
    \item Training accuracy: 97.13\% vs Validation accuracy: 38.12\%
    \item Gap of \textbf{59.01\%} indicates poor generalization
    \item Validation performance near random baseline (33\% for 3 classes)
\end{itemize}

\subsection{Class Imbalance Issues}
\begin{itemize}
    \item \textbf{Car class}: Best validation performance (81.5\% recall, 50\% F1)
    \item \textbf{Bicycle class}: Worst performance (7.5\% recall, 12.6\% F1)
    \item \textbf{1\_person class}: Moderate performance (25.2\% recall, 32.5\% F1)
\end{itemize}

\section{Recommendations}

\subsection{Immediate Actions}
\begin{enumerate}
    \item \textbf{Increase Regularization}:
        \begin{itemize}
            \item Dropout: 0.5 → 0.7
            \item Weight decay: 0.01 → 0.05
            \item Add batch normalization
        \end{itemize}
    
    \item \textbf{Data Augmentation Enhancement}:
        \begin{itemize}
            \item Increase rotation range: ±15° → ±25°
            \item Add Gaussian noise: σ=0.02
            \item Implement mixup augmentation
        \end{itemize}
    
    \item \textbf{Training Strategy}:
        \begin{itemize}
            \item Reduce learning rate: 2e-4 → 5e-5
            \item Extend training: 5 → 15 epochs
            \item Implement early stopping based on validation loss
        \end{itemize}
\end{enumerate}

\subsection{Architecture Improvements}
\begin{enumerate}
    \item \textbf{Model Complexity Reduction}:
        \begin{itemize}
            \item Replace EfficientNet-B3 with EfficientNet-B0
            \item Reduce LSTM hidden units: 384 → 128
            \item Add skip connections in classifier
        \end{itemize}
    
    \item \textbf{Feature Engineering}:
        \begin{itemize}
            \item Investigate CSV feature distributions
            \item Apply feature normalization/standardization
            \item Consider feature selection techniques
        \end{itemize}
\end{enumerate}

\section{Technical Infrastructure}

\subsection{CI/CD Pipeline Success}
\begin{itemize}
    \item \textbf{Platform}: GitHub Actions (ubuntu-latest)
    \item \textbf{Duration}: 21.8 minutes for 5 epochs
    \item \textbf{Resource Usage}: 8GB memory, 2 CPU cores
    \item \textbf{Data Management}: DVC + DagsHub integration
    \item \textbf{Model Tracking}: MLflow with comprehensive metrics
\end{itemize}

\subsection{Reproducibility}
\begin{itemize}
    \item \textbf{MLflow Run ID}: \texttt{05031104766646aa8e03a3cac5aa48ae}
    \item \textbf{Git Commit}: \texttt{73d9c630d7b723ea113fa62fe3acd7f8d46924b3}
    \item \textbf{Environment}: Python 3.10, PyTorch 2.0.1
    \item \textbf{Seed}: Fixed for reproducible results
\end{itemize}

\section{Conclusion}

The trimodal radar classification pipeline successfully demonstrates:
\begin{itemize}
    \item \textbf{Technical Feasibility}: Complete CI/CD automation
    \item \textbf{Model Integration}: Successful fusion of three data modalities
    \item \textbf{Infrastructure Scalability}: Production-ready deployment pipeline
\end{itemize}

However, the severe overfitting issue requires immediate attention before deployment. The model achieves excellent training performance but fails to generalize, indicating the need for stronger regularization and potentially simpler architecture.

\textbf{Next Steps}: Implement the recommended regularization strategies and retrain with extended epochs to achieve better generalization performance.

\end{document}